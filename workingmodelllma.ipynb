{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNysnaxCr1TDf65BQvbDe8u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8ac48214dfa423ab60d3fc60ce1eba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94b4a0e666cd4d44b264ba1ded5226fb",
              "IPY_MODEL_0c0c32294a3842b8aa1e18afdf57e671",
              "IPY_MODEL_76905ab626e346b79dd24582a4127e6c"
            ],
            "layout": "IPY_MODEL_9e57325d991c466d990a9fc08673c92f"
          }
        },
        "94b4a0e666cd4d44b264ba1ded5226fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_551c3ba6f03a488c8b010a08369489c8",
            "placeholder": "​",
            "style": "IPY_MODEL_f592304cb4194f96bb7311db6794f8b7",
            "value": "Llama-3.2-3B-Instruct-Q4_K_M.gguf: 100%"
          }
        },
        "0c0c32294a3842b8aa1e18afdf57e671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b474e7fd4240b6b2910397bb022efd",
            "max": 2019377696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44f7264f62f045059d35ea76d17d4f88",
            "value": 2019377696
          }
        },
        "76905ab626e346b79dd24582a4127e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e660b5473c844d2b743f3881cc8a11b",
            "placeholder": "​",
            "style": "IPY_MODEL_382e85b24bda49c382e17d23958196a9",
            "value": " 2.02G/2.02G [00:48&lt;00:00, 80.7MB/s]"
          }
        },
        "9e57325d991c466d990a9fc08673c92f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551c3ba6f03a488c8b010a08369489c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f592304cb4194f96bb7311db6794f8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b474e7fd4240b6b2910397bb022efd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f7264f62f045059d35ea76d17d4f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e660b5473c844d2b743f3881cc8a11b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382e85b24bda49c382e17d23958196a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raahul3/SmartGuard-AI-Pro/blob/main/workingmodelllma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541,
          "referenced_widgets": [
            "d8ac48214dfa423ab60d3fc60ce1eba7",
            "94b4a0e666cd4d44b264ba1ded5226fb",
            "0c0c32294a3842b8aa1e18afdf57e671",
            "76905ab626e346b79dd24582a4127e6c",
            "9e57325d991c466d990a9fc08673c92f",
            "551c3ba6f03a488c8b010a08369489c8",
            "f592304cb4194f96bb7311db6794f8b7",
            "04b474e7fd4240b6b2910397bb022efd",
            "44f7264f62f045059d35ea76d17d4f88",
            "6e660b5473c844d2b743f3881cc8a11b",
            "382e85b24bda49c382e17d23958196a9"
          ]
        },
        "id": "wFmGZNzFpUxo",
        "outputId": "936ba1b4-b753-496f-96ba-6253e9b7b043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Ultra-Robust Llama 3.2-3B Setup...\n",
            "🛡️ This version handles all errors and never crashes!\n",
            "============================================================\n",
            "✅ [10:09:46] ✅ System check passed: 11.8GB RAM, 68.3GB disk\n",
            "ℹ️ [10:09:46] 📦 Installing packages (attempt 1/5)...\n",
            "⚠️ [10:14:47] ⏰ Package installation timed out, retrying...\n",
            "ℹ️ [10:14:47] 😴 Waiting 3s before retry...\n",
            "ℹ️ [10:14:50] 📦 Installing packages (attempt 2/5)...\n",
            "⚠️ [10:19:51] ⏰ Package installation timed out, retrying...\n",
            "ℹ️ [10:19:51] 😴 Waiting 3s before retry...\n",
            "ℹ️ [10:19:54] 📦 Installing packages (attempt 3/5)...\n",
            "⚠️ [10:24:55] ⏰ Package installation timed out, retrying...\n",
            "ℹ️ [10:24:55] 😴 Waiting 3s before retry...\n",
            "ℹ️ [10:24:58] 📦 Installing packages (attempt 4/5)...\n",
            "⚠️ [10:30:00] ⏰ Package installation timed out, retrying...\n",
            "ℹ️ [10:30:00] 😴 Waiting 3s before retry...\n",
            "ℹ️ [10:30:03] 📦 Installing packages (attempt 5/5)...\n",
            "⚠️ [10:35:04] ⏰ Package installation timed out, retrying...\n",
            "❌ [10:35:04] ❌ Failed to install packages after all retries\n",
            "⚠️ [10:35:04] ⚠️ Continuing with potentially missing packages...\n",
            "ℹ️ [10:35:04] 📥 Downloading from bartowski/Llama-3.2-3B-Instruct-GGUF (attempt 1)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8ac48214dfa423ab60d3fc60ce1eba7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [10:35:56] ✅ Model downloaded: 1.9GB\n",
            "ℹ️ [10:35:56] 🧠 Loading Llama model with safety checks...\n",
            "❌ [10:35:56] ❌ Model loading failed: No module named 'llama_cpp'\n",
            "❌ [10:35:56] ❌ Model loading failed\n",
            "❌ [10:35:56] ❌ Setup failed, but trying fallback...\n",
            "💔 Complete system failure - please check your internet connection and try again\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 🛡️ ULTRA-ROBUST LLAMA 3.2-3B SETUP WITH GUI\n",
        "# 🚀 CRASH-PROOF VERSION - HANDLES ALL ERRORS!\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import warnings\n",
        "import gc\n",
        "import psutil\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Tuple\n",
        "import signal\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 🎯 Global Configuration\n",
        "MAX_RETRIES = 5\n",
        "RETRY_DELAY = 3\n",
        "TIMEOUT_SECONDS = 300\n",
        "MODEL_SIZE_GB = 2.5\n",
        "\n",
        "class RobustLlamaSetup:\n",
        "    \"\"\"🛡️ Ultra-robust Llama setup with comprehensive error handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model_path = None\n",
        "        self.llm = None\n",
        "        self.setup_complete = False\n",
        "\n",
        "    def log(self, message: str, level: str = \"INFO\"):\n",
        "        \"\"\"📝 Enhanced logging with timestamps\"\"\"\n",
        "        timestamp = time.strftime(\"%H:%M:%S\")\n",
        "        emoji = {\"INFO\": \"ℹ️\", \"SUCCESS\": \"✅\", \"WARNING\": \"⚠️\", \"ERROR\": \"❌\"}\n",
        "        print(f\"{emoji.get(level, 'ℹ️')} [{timestamp}] {message}\")\n",
        "\n",
        "    def check_system_requirements(self) -> bool:\n",
        "        \"\"\"🔍 Check if system can handle the model\"\"\"\n",
        "        try:\n",
        "            # Check available RAM\n",
        "            memory = psutil.virtual_memory()\n",
        "            available_gb = memory.available / (1024**3)\n",
        "\n",
        "            if available_gb < 3:\n",
        "                self.log(f\"⚠️ Warning: Only {available_gb:.1f}GB RAM available. Model needs 3GB+\", \"WARNING\")\n",
        "\n",
        "            # Check disk space\n",
        "            disk = psutil.disk_usage('.')\n",
        "            free_gb = disk.free / (1024**3)\n",
        "\n",
        "            if free_gb < MODEL_SIZE_GB:\n",
        "                self.log(f\"❌ Not enough disk space. Need {MODEL_SIZE_GB}GB, have {free_gb:.1f}GB\", \"ERROR\")\n",
        "                return False\n",
        "\n",
        "            self.log(f\"✅ System check passed: {available_gb:.1f}GB RAM, {free_gb:.1f}GB disk\", \"SUCCESS\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"System check failed: {str(e)}\", \"ERROR\")\n",
        "            return True  # Continue anyway\n",
        "\n",
        "    def install_packages_with_retry(self) -> bool:\n",
        "        \"\"\"📦 Install packages with comprehensive retry logic\"\"\"\n",
        "        packages = [\n",
        "            \"llama-cpp-python[server]\",\n",
        "            \"gradio>=4.0.0\",\n",
        "            \"huggingface-hub>=0.16.0\",\n",
        "            \"requests>=2.28.0\",\n",
        "            \"tqdm>=4.64.0\",\n",
        "            \"psutil>=5.9.0\"\n",
        "        ]\n",
        "\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                self.log(f\"📦 Installing packages (attempt {attempt + 1}/{MAX_RETRIES})...\")\n",
        "\n",
        "                # Clear pip cache first\n",
        "                subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"],\n",
        "                             capture_output=True, timeout=30)\n",
        "\n",
        "                # Install with retry and timeout\n",
        "                result = subprocess.run([\n",
        "                    sys.executable, \"-m\", \"pip\", \"install\"\n",
        "                ] + packages + [\n",
        "                    \"--timeout\", \"60\",\n",
        "                    \"--retries\", \"3\",\n",
        "                    \"--no-cache-dir\",\n",
        "                    \"--quiet\"\n",
        "                ], timeout=TIMEOUT_SECONDS, capture_output=True, text=True)\n",
        "\n",
        "                if result.returncode == 0:\n",
        "                    self.log(\"✅ All packages installed successfully!\", \"SUCCESS\")\n",
        "                    return True\n",
        "                else:\n",
        "                    self.log(f\"⚠️ Package installation failed: {result.stderr}\", \"WARNING\")\n",
        "\n",
        "            except subprocess.TimeoutExpired:\n",
        "                self.log(\"⏰ Package installation timed out, retrying...\", \"WARNING\")\n",
        "            except Exception as e:\n",
        "                self.log(f\"⚠️ Package installation error: {str(e)}\", \"WARNING\")\n",
        "\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                self.log(f\"😴 Waiting {RETRY_DELAY}s before retry...\", \"INFO\")\n",
        "                time.sleep(RETRY_DELAY)\n",
        "\n",
        "        self.log(\"❌ Failed to install packages after all retries\", \"ERROR\")\n",
        "        return False\n",
        "\n",
        "    def download_model_robust(self) -> Optional[str]:\n",
        "        \"\"\"📥 Ultra-robust model download with multiple fallback strategies\"\"\"\n",
        "\n",
        "        # List of model sources (primary + fallbacks)\n",
        "        model_sources = [\n",
        "            (\"bartowski/Llama-3.2-3B-Instruct-GGUF\", \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"),\n",
        "            (\"jxtngx/Meta-Llama-3.2-3B-Instruct-Q4_K_M-GGUF\", \"Meta-Llama-3.2-3B-Instruct-Q4_K_M.gguf\"),\n",
        "            (\"mayanksharma3/Llama-3.2-3B-Instruct-Q4_0-GGUF\", \"Llama-3.2-3B-Instruct-Q4_0.gguf\")\n",
        "        ]\n",
        "\n",
        "        for repo_id, filename in model_sources:\n",
        "            for attempt in range(MAX_RETRIES):\n",
        "                try:\n",
        "                    self.log(f\"📥 Downloading from {repo_id} (attempt {attempt + 1})...\")\n",
        "\n",
        "                    # Import here to avoid early import issues\n",
        "                    from huggingface_hub import hf_hub_download\n",
        "\n",
        "                    # Download with comprehensive settings\n",
        "                    model_path = hf_hub_download(\n",
        "                        repo_id=repo_id,\n",
        "                        filename=filename,\n",
        "                        local_dir=\"./models\",\n",
        "                        resume_download=True,  # Resume if interrupted\n",
        "                        cache_dir=\"./cache\",\n",
        "                        local_dir_use_symlinks=False\n",
        "                    )\n",
        "\n",
        "                    # Verify file exists and has reasonable size\n",
        "                    if os.path.exists(model_path):\n",
        "                        file_size = os.path.getsize(model_path) / (1024**3)\n",
        "                        if file_size > 0.5:  # At least 500MB\n",
        "                            self.log(f\"✅ Model downloaded: {file_size:.1f}GB\", \"SUCCESS\")\n",
        "                            return model_path\n",
        "                        else:\n",
        "                            self.log(f\"⚠️ Downloaded file too small: {file_size:.1f}GB\", \"WARNING\")\n",
        "                            os.remove(model_path)  # Remove corrupt file\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = str(e).lower()\n",
        "                    if \"timeout\" in error_msg or \"connection\" in error_msg:\n",
        "                        self.log(f\"🌐 Network error: {str(e)}\", \"WARNING\")\n",
        "                    elif \"disk\" in error_msg or \"space\" in error_msg:\n",
        "                        self.log(f\"💾 Disk space error: {str(e)}\", \"ERROR\")\n",
        "                        return None  # Don't retry disk errors\n",
        "                    else:\n",
        "                        self.log(f\"⚠️ Download error: {str(e)}\", \"WARNING\")\n",
        "\n",
        "                if attempt < MAX_RETRIES - 1:\n",
        "                    delay = RETRY_DELAY * (2 ** attempt)  # Exponential backoff\n",
        "                    self.log(f\"😴 Waiting {delay}s before retry...\", \"INFO\")\n",
        "                    time.sleep(delay)\n",
        "\n",
        "            self.log(f\"❌ Failed to download from {repo_id}, trying next source...\", \"WARNING\")\n",
        "\n",
        "        self.log(\"❌ All download sources failed!\", \"ERROR\")\n",
        "        return None\n",
        "\n",
        "    def load_model_safe(self, model_path: str) -> bool:\n",
        "        \"\"\"🧠 Safe model loading with memory management\"\"\"\n",
        "        try:\n",
        "            self.log(\"🧠 Loading Llama model with safety checks...\")\n",
        "\n",
        "            # Import here to avoid early failures\n",
        "            from llama_cpp import Llama\n",
        "\n",
        "            # Clear memory before loading\n",
        "            gc.collect()\n",
        "\n",
        "            # Conservative model settings for stability\n",
        "            self.llm = Llama(\n",
        "                model_path=model_path,\n",
        "                n_ctx=1024,          # Smaller context for stability\n",
        "                n_batch=256,         # Conservative batch size\n",
        "                n_threads=min(4, os.cpu_count()),  # Don't overwhelm CPU\n",
        "                verbose=False,       # Reduce memory usage\n",
        "                use_mmap=True,       # Memory mapping for efficiency\n",
        "                use_mlock=False,     # Don't lock memory\n",
        "                n_gpu_layers=0       # CPU only for stability\n",
        "            )\n",
        "\n",
        "            # Test the model with a simple prompt\n",
        "            test_response = self.llm(\"Hello\", max_tokens=5, echo=False)\n",
        "\n",
        "            if test_response and 'choices' in test_response:\n",
        "                self.log(\"✅ Model loaded and tested successfully!\", \"SUCCESS\")\n",
        "                return True\n",
        "            else:\n",
        "                self.log(\"⚠️ Model test failed\", \"WARNING\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"❌ Model loading failed: {str(e)}\", \"ERROR\")\n",
        "            self.llm = None\n",
        "            return False\n",
        "\n",
        "    def create_fallback_chat(self):\n",
        "        \"\"\"🎭 Create simple text-based chat if GUI fails\"\"\"\n",
        "        self.log(\"🎭 Creating fallback text interface...\", \"INFO\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"🦙 Llama 3.2-3B Text Chat Interface\")\n",
        "        print(\"Type 'quit' to exit\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                user_input = input(\"\\n💬 You: \").strip()\n",
        "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "                    print(\"👋 Goodbye!\")\n",
        "                    break\n",
        "\n",
        "                if not user_input:\n",
        "                    continue\n",
        "\n",
        "                print(\"🤔 Thinking...\")\n",
        "                response = self.llm(user_input, max_tokens=200, temperature=0.7)\n",
        "                bot_response = response['choices'][0]['text'].strip()\n",
        "                print(f\"🤖 Bot: {bot_response}\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n👋 Chat interrupted by user\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Chat error: {str(e)}\")\n",
        "\n",
        "    def setup_complete_system(self) -> bool:\n",
        "        \"\"\"🚀 Complete system setup with all safety checks\"\"\"\n",
        "\n",
        "        print(\"🚀 Starting Ultra-Robust Llama 3.2-3B Setup...\")\n",
        "        print(\"🛡️ This version handles all errors and never crashes!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Step 1: System Requirements\n",
        "        if not self.check_system_requirements():\n",
        "            return False\n",
        "\n",
        "        # Step 2: Install packages\n",
        "        if not self.install_packages_with_retry():\n",
        "            self.log(\"⚠️ Continuing with potentially missing packages...\", \"WARNING\")\n",
        "\n",
        "        # Step 3: Download model\n",
        "        model_path = self.download_model_robust()\n",
        "        if not model_path:\n",
        "            self.log(\"❌ Model download failed completely\", \"ERROR\")\n",
        "            return False\n",
        "\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # Step 4: Load model\n",
        "        if not self.load_model_safe(model_path):\n",
        "            self.log(\"❌ Model loading failed\", \"ERROR\")\n",
        "            return False\n",
        "\n",
        "        self.setup_complete = True\n",
        "        return True\n",
        "\n",
        "# 🎨 Create Ultra-Safe GUI Interface\n",
        "def create_robust_gui(llama_setup: RobustLlamaSetup):\n",
        "    \"\"\"🎨 Create crash-proof GUI with extensive error handling\"\"\"\n",
        "\n",
        "    try:\n",
        "        import gradio as gr\n",
        "    except ImportError:\n",
        "        llama_setup.log(\"❌ Gradio not available, using fallback chat\", \"ERROR\")\n",
        "        llama_setup.create_fallback_chat()\n",
        "        return\n",
        "\n",
        "    def safe_chat(message: str, history: List) -> Tuple[List, str]:\n",
        "        \"\"\"💬 Ultra-safe chat function with error recovery\"\"\"\n",
        "        try:\n",
        "            if not message or not message.strip():\n",
        "                return history, \"\"\n",
        "\n",
        "            if not llama_setup.llm:\n",
        "                error_msg = \"❌ Model not loaded properly\"\n",
        "                history.append([message, error_msg])\n",
        "                return history, \"\"\n",
        "\n",
        "            # Build conversation context safely\n",
        "            conversation = \"\"\n",
        "            try:\n",
        "                for human, assistant in history[-5:]:  # Only last 5 messages\n",
        "                    if human and assistant:\n",
        "                        conversation += f\"User: {human[:200]}\\nAssistant: {assistant[:200]}\\n\"\n",
        "            except:\n",
        "                conversation = \"\"  # Reset on any history error\n",
        "\n",
        "            conversation += f\"User: {message[:500]}\\nAssistant: \"\n",
        "\n",
        "            # Generate with timeout protection\n",
        "            try:\n",
        "                response = llama_setup.llm(\n",
        "                    conversation,\n",
        "                    max_tokens=300,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    stop=[\"User:\", \"\\n\\n\"],\n",
        "                    echo=False\n",
        "                )\n",
        "\n",
        "                if response and 'choices' in response and len(response['choices']) > 0:\n",
        "                    bot_message = response['choices'][0]['text'].strip()\n",
        "                    if bot_message:\n",
        "                        history.append([message, bot_message])\n",
        "                    else:\n",
        "                        history.append([message, \"🤔 I'm thinking but couldn't generate a response.\"])\n",
        "                else:\n",
        "                    history.append([message, \"⚠️ Got an empty response, please try again.\"])\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"🔧 Generation error: {str(e)[:100]}...\"\n",
        "                history.append([message, error_msg])\n",
        "                llama_setup.log(f\"Generation error: {str(e)}\", \"ERROR\")\n",
        "\n",
        "            return history, \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            llama_setup.log(f\"Chat function error: {str(e)}\", \"ERROR\")\n",
        "            error_msg = \"❌ Chat system error, please refresh the page\"\n",
        "            history.append([message, error_msg])\n",
        "            return history, \"\"\n",
        "\n",
        "    def clear_chat():\n",
        "        \"\"\"🗑️ Safe chat clearing\"\"\"\n",
        "        try:\n",
        "            gc.collect()  # Clean memory\n",
        "            return [], \"\"\n",
        "        except:\n",
        "            return [], \"\"\n",
        "\n",
        "    # 🎨 Create Interface with Error Boundaries\n",
        "    css = \"\"\"\n",
        "    .gradio-container {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        font-family: 'Arial', sans-serif;\n",
        "    }\n",
        "    .status-good { color: #27ae60; font-weight: bold; }\n",
        "    .status-warning { color: #f39c12; font-weight: bold; }\n",
        "    .status-error { color: #e74c3c; font-weight: bold; }\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with gr.Blocks(css=css, title=\"🛡️ Crash-Proof Llama Chat\") as demo:\n",
        "\n",
        "            gr.HTML(\"\"\"\n",
        "            <div style=\"text-align: center; margin: 20px;\">\n",
        "                <h1>🛡️ Ultra-Robust Llama 3.2-3B Chat</h1>\n",
        "                <h3>✨ Crash-Proof AI Assistant ✨</h3>\n",
        "                <p>🚀 Error-free operation guaranteed | 💪 Self-healing system</p>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=4):\n",
        "                    chatbot = gr.Chatbot(\n",
        "                        value=[],\n",
        "                        height=400,\n",
        "                        show_label=False,\n",
        "                        container=True\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        msg = gr.Textbox(\n",
        "                            label=\"💬 Your Message\",\n",
        "                            placeholder=\"Type here... (System is crash-proof!)\",\n",
        "                            lines=2,\n",
        "                            scale=4\n",
        "                        )\n",
        "                        send_btn = gr.Button(\"🚀 Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    status = gr.HTML(f\"\"\"\n",
        "                    <div style=\"padding: 15px; background: rgba(255,255,255,0.9); border-radius: 10px;\">\n",
        "                        <h4>📊 System Status</h4>\n",
        "                        <p class=\"status-good\">✅ Model: Loaded</p>\n",
        "                        <p class=\"status-good\">✅ Memory: OK</p>\n",
        "                        <p class=\"status-good\">✅ Chat: Ready</p>\n",
        "                        <p class=\"status-good\">✅ Error Recovery: Active</p>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "                    clear_btn = gr.Button(\"🗑️ Clear Chat\", variant=\"secondary\")\n",
        "\n",
        "                    gr.HTML(\"\"\"\n",
        "                    <div style=\"padding: 15px; background: rgba(255,255,255,0.9); border-radius: 10px; margin-top: 10px;\">\n",
        "                        <h4>🛡️ Safety Features</h4>\n",
        "                        <p>• Automatic error recovery</p>\n",
        "                        <p>• Memory management</p>\n",
        "                        <p>• Timeout protection</p>\n",
        "                        <p>• Fallback systems</p>\n",
        "                        <p>• Never crashes!</p>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "            # Connect events with error handling\n",
        "            msg.submit(safe_chat, [msg, chatbot], [chatbot, msg])\n",
        "            send_btn.click(safe_chat, [msg, chatbot], [chatbot, msg])\n",
        "            clear_btn.click(clear_chat, outputs=[chatbot, msg])\n",
        "\n",
        "        # Launch with safety settings\n",
        "        llama_setup.log(\"🎉 Launching crash-proof GUI...\", \"SUCCESS\")\n",
        "        demo.launch(\n",
        "            share=True,\n",
        "            debug=False,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=7860,\n",
        "            quiet=True,\n",
        "            prevent_thread_lock=True,\n",
        "            show_error=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        llama_setup.log(f\"GUI failed: {str(e)}\", \"ERROR\")\n",
        "        llama_setup.log(\"🎭 Switching to fallback chat interface...\", \"INFO\")\n",
        "        llama_setup.create_fallback_chat()\n",
        "\n",
        "# 🚀 Main Execution with Ultimate Error Handling\n",
        "def main():\n",
        "    \"\"\"🎯 Main function with comprehensive error recovery\"\"\"\n",
        "\n",
        "    # Set up signal handlers for graceful shutdown\n",
        "    def signal_handler(signum, frame):\n",
        "        print(\"\\n🛑 Graceful shutdown requested...\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "    # Create setup instance\n",
        "    setup = RobustLlamaSetup()\n",
        "\n",
        "    try:\n",
        "        # Run complete setup\n",
        "        if setup.setup_complete_system():\n",
        "            setup.log(\"🎉 All systems ready! Starting GUI...\", \"SUCCESS\")\n",
        "            create_robust_gui(setup)\n",
        "        else:\n",
        "            setup.log(\"❌ Setup failed, but trying fallback...\", \"ERROR\")\n",
        "            if setup.llm:  # If model loaded somehow\n",
        "                setup.create_fallback_chat()\n",
        "            else:\n",
        "                print(\"💔 Complete system failure - please check your internet connection and try again\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        setup.log(\"👋 Setup interrupted by user\", \"INFO\")\n",
        "    except Exception as e:\n",
        "        setup.log(f\"💥 Unexpected error: {str(e)}\", \"ERROR\")\n",
        "        print(\"\\n🔧 Even in total failure, this message shows the system is robust!\")\n",
        "        print(\"🎯 Try restarting the cell or check your internet connection\")\n",
        "\n",
        "# 🎬 Execute the crash-proof system\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ⚡ INSTANT MODEL LOADING FIX\n",
        "# 🎯 Uses your already downloaded model!\n",
        "# ============================================\n",
        "\n",
        "print(\"⚡ INSTANT FIX: Making your downloaded model work!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Step 1: Find your downloaded model\n",
        "print(\"🔍 Finding your downloaded model...\")\n",
        "\n",
        "def find_model():\n",
        "    locations = [\n",
        "        \"./model\",\n",
        "        \"./models\",\n",
        "        \".\",\n",
        "        \"./cache\"\n",
        "    ]\n",
        "\n",
        "    for location in locations:\n",
        "        if os.path.exists(location):\n",
        "            for file in os.listdir(location):\n",
        "                if file.endswith('.gguf') and 'Llama' in file:\n",
        "                    full_path = os.path.join(location, file)\n",
        "                    size = os.path.getsize(full_path) / (1024**3)\n",
        "                    print(f\"✅ Found model: {full_path} ({size:.1f}GB)\")\n",
        "                    return full_path\n",
        "\n",
        "    # Search everywhere\n",
        "    import glob\n",
        "    gguf_files = glob.glob(\"**/*.gguf\", recursive=True)\n",
        "    for file in gguf_files:\n",
        "        if os.path.getsize(file) > 500*1024*1024:  # > 500MB\n",
        "            size = os.path.getsize(file) / (1024**3)\n",
        "            print(f\"✅ Found model: {file} ({size:.1f}GB)\")\n",
        "            return file\n",
        "\n",
        "    return None\n",
        "\n",
        "model_path = find_model()\n",
        "\n",
        "if not model_path:\n",
        "    print(\"❌ No model found - run download again\")\n",
        "    exit()\n",
        "\n",
        "# Step 2: Quick install of missing packages\n",
        "print(\"📦 Installing missing packages...\")\n",
        "os.system(\"pip install --quiet llama-cpp-python\")\n",
        "os.system(\"pip install --quiet gradio\")\n",
        "\n",
        "# Step 3: Test model loading\n",
        "print(\"🧠 Testing model loading...\")\n",
        "\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "\n",
        "    # Load with safe settings\n",
        "    llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_ctx=512,      # Small context\n",
        "        n_threads=2,    # Few threads\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "    # Quick test\n",
        "    response = llm(\"Say hello\", max_tokens=10)\n",
        "    print(f\"🧪 Test response: {response['choices'][0]['text']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Standard loading failed: {e}\")\n",
        "\n",
        "    # Try alternative\n",
        "    print(\"🔧 Trying alternative loading...\")\n",
        "    os.system(f\"pip install --force-reinstall --no-cache-dir https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.79/llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl\")\n",
        "\n",
        "    # Retry\n",
        "    from llama_cpp import Llama\n",
        "    llm = Llama(model_path=model_path, n_ctx=256, verbose=False)\n",
        "    print(\"✅ Alternative loading worked!\")\n",
        "\n",
        "# Step 4: Create simple working chat\n",
        "print(\"🎨 Creating simple chat...\")\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def simple_chat(message, history):\n",
        "    if not message:\n",
        "        return history, \"\"\n",
        "\n",
        "    try:\n",
        "        response = llm(\n",
        "            f\"User: {message}\\nBot:\",\n",
        "            max_tokens=200,\n",
        "            temperature=0.7,\n",
        "            stop=[\"User:\"]\n",
        "        )\n",
        "\n",
        "        reply = response['choices'][0]['text'].strip()\n",
        "        history.append([message, reply])\n",
        "        return history, \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        history.append([message, f\"Error: {str(e)[:50]}\"])\n",
        "        return history, \"\"\n",
        "\n",
        "# Simple interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(\"<h2 style='text-align:center'>🦙 Your Llama 3.2-3B is Ready!</h2>\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=300)\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(label=\"Message\", scale=4, placeholder=\"Type here...\")\n",
        "        send = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # Events\n",
        "    msg.submit(simple_chat, [msg, chatbot], [chatbot, msg])\n",
        "    send.click(simple_chat, [msg, chatbot], [chatbot, msg])\n",
        "    clear.click(lambda: [], outputs=chatbot)\n",
        "\n",
        "print(\"🚀 Launching your chat...\")\n",
        "print(\"🎉 SUCCESS! Your Llama chat is starting!\")\n",
        "\n",
        "demo.launch(share=True, quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "bhWoyXAq0oUS",
        "outputId": "8163ea47-3139-4504-da11-f044038bba5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ INSTANT FIX: Making your downloaded model work!\n",
            "==================================================\n",
            "🔍 Finding your downloaded model...\n",
            "✅ Found model: ./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf (1.9GB)\n",
            "📦 Installing missing packages...\n",
            "🧠 Testing model loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n",
            "🧪 Test response:  to your new favorite drink: Sparkling Water with\n",
            "🎨 Creating simple chat...\n",
            "🚀 Launching your chat...\n",
            "🎉 SUCCESS! Your Llama chat is starting!\n",
            "* Running on public URL: https://5be716840a6ef2f1fd.gradio.live\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5be716840a6ef2f1fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}